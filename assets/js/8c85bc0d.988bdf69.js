"use strict";(self.webpackChunkphysical_ai_book=self.webpackChunkphysical_ai_book||[]).push([[748],{4264(n,e,i){i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-8-vla","title":"Chapter 8: Vision-Language-Action VLA & Conversational Robotics","description":"What is VLA?","source":"@site/docs/chapter-8-vla.md","sourceDirName":".","slug":"/chapter-8-vla","permalink":"/physical-ai-book/chapter-8-vla","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"mySidebar","previous":{"title":"Chapter 7: NVIDIA Isaac AI & Robot Brain","permalink":"/physical-ai-book/chapter-7-nvidia-isaac"},"next":{"title":"Chapter 9: Summary, Hardware, and Lab Setup","permalink":"/physical-ai-book/chapter-9-hardware-lab"}}');var t=i(4848),o=i(8453);const a={},l="Chapter 8: Vision-Language-Action VLA & Conversational Robotics",r={},c=[{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Voice-to-Action",id:"voice-to-action",level:2},{value:"Cognitive Planning",id:"cognitive-planning",level:2},{value:"Multi-Modal Interaction",id:"multi-modal-interaction",level:2},{value:"Capstone Idea",id:"capstone-idea",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-8-vision-language-action-vla--conversational-robotics",children:"Chapter 8: Vision-Language-Action VLA & Conversational Robotics"})}),"\n",(0,t.jsx)(e.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["VLA combines:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision:"})," Robot sees objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language:"})," Robot understands spoken commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action:"})," Robot performs tasks"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:'Example: "Pick up the red box" \u2192 Robot sees box, plans action, picks it up'}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"voice-to-action",children:"Voice-to-Action"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Robots can use ",(0,t.jsx)(e.strong,{children:"OpenAI Whisper"})," or other tools to understand voice commands"]}),"\n",(0,t.jsxs)(e.li,{children:["Converts words into ",(0,t.jsx)(e.strong,{children:"robot actions"})]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Robots use AI to ",(0,t.jsx)(e.strong,{children:"decide what to do step by step"})]}),"\n",(0,t.jsx)(e.li,{children:"Example: Clean the room \u2192 Plan to pick objects, move them, put them away"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"multi-modal-interaction",children:"Multi-Modal Interaction"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Robot can use:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Speech"})," \u2192 Talk to humans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gesture"})," \u2192 Detect human movements"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision"})," \u2192 See objects and obstacles"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"capstone-idea",children:"Capstone Idea"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Build a robot that:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Receives a voice command"}),"\n",(0,t.jsx)(e.li,{children:"Plans a path"}),"\n",(0,t.jsx)(e.li,{children:"Navigates obstacles"}),"\n",(0,t.jsx)(e.li,{children:"Identifies an object"}),"\n",(0,t.jsx)(e.li,{children:"Picks and places it"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"VLA = see, understand, act"}),"\n",(0,t.jsx)(e.li,{children:"Voice-to-Action converts speech to tasks"}),"\n",(0,t.jsx)(e.li,{children:"Multi-modal interaction makes robots smarter"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>l});var s=i(6540);const t={},o=s.createContext(t);function a(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);